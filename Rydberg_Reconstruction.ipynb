{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we import the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from qucumber.nn_states import PositiveWaveFunction\n",
    "from qucumber.callbacks import MetricEvaluator\n",
    "\n",
    "import qucumber.utils.training_statistics as ts\n",
    "import qucumber.utils.data as data\n",
    "import qucumber\n",
    "\n",
    "import torch\n",
    "\n",
    "# set random seed on cpu but not gpu, as gpu is not used\n",
    "qucumber.set_random_seed(1234, cpu=True, gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 8])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we get the training data\n",
    "# Training data is given in the data/nY=8/ directory for each delta\n",
    "# in the '...samples.csv' files.\n",
    "# So the delta needs to be specified below\n",
    "\n",
    "#-------------------- Specify delta ----------------------------#\n",
    "delta = \"1.04\" \n",
    "# delta is any of 1.00, 1.02, 1.04, 1.06, 1.08, 1.10, 1.12, 1.14, 1.16, 1.18, 1.20\n",
    "#---------------------------------------------------------------#\n",
    "\n",
    "train_path = \"data/nY=8/δ=\"+delta+\"_samples.csv\"\n",
    "train_data = data.load_data(train_path)\n",
    "\n",
    "# The training data is stored in train_data[0]\n",
    "# and the dimension of the data is:\n",
    "train_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now we have the training data. Our goal is to train a\n",
    "# Restricted Boltzman Machine using this training data. We use\n",
    "# QuCumber to create an instance of an RBM. First we need to specify the \n",
    "# number of visible nodes (nv) and number of higgen nodes (nh). \n",
    "# As we have an array of 8 atoms, we have 6 inputs and so nv = 8\n",
    "nv = train_data[0].shape[-1]\n",
    "\n",
    "# Number of hidden nodes of an RBM is a hyperparameter which depends on the \n",
    "# data we are using and which can be varied to get optimal result. \n",
    "# For this problem, nh was varied from 4 to 8 and nh = 8 gave quite good results.\n",
    "# Hence, we set nh = 8.\n",
    "nh = 8\n",
    "\n",
    "# Finally we create an RBM with nv visible nodes and nh  hidden nodes.\n",
    "nn_state = PositiveWaveFunction(num_visible=nv, num_hidden=nh, gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time elapsed during training: 176.272 s\n"
     ]
    }
   ],
   "source": [
    "# Below we have more hyperparameters of the RBM model. Like the number of \n",
    "# hidden nodes, the parameters below can be varied to get optimal results.\n",
    "# The following hyperparameters seemed to work quite well. Note that when\n",
    "# you have different data, then different values of the hyperparameter will give\n",
    "# better solutions. \n",
    "# Further description of the parameters (if you are curious) is in:\n",
    "# https://qucumber.readthedocs.io/en/stable/quantum_states.html?highlight=fit#qucumber.nn_states.PositiveWaveFunction.fit\n",
    "\n",
    "epochs = 500\n",
    "pbs = 100\n",
    "nbs = pbs\n",
    "lr = 0.0065 \n",
    "k = 10\n",
    "\n",
    "# Now we train our RBM using the above parameters. \n",
    "# Note that the training process will take 2 - 3 minutes\n",
    "\n",
    "nn_state.fit(\n",
    "    train_data[0],\n",
    "    epochs=epochs,\n",
    "    pos_batch_size=pbs,\n",
    "    neg_batch_size=nbs,\n",
    "    lr=lr,\n",
    "    k=k,\n",
    "    time=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rbm_am': OrderedDict([('weights',\n",
       "               tensor([[-0.2691, -0.1283,  0.3138, -0.4687,  0.3145, -0.4566,  0.0781, -0.0145],\n",
       "                       [-0.2504,  0.1024, -0.3418,  0.3027, -0.3777, -0.1798,  0.1818, -0.0577],\n",
       "                       [-0.2438,  0.3075, -0.1975, -1.0659,  1.5449, -1.6659,  0.7075, -0.3233],\n",
       "                       [-0.6828,  0.2191, -0.2113,  0.0933, -0.4902,  0.5794, -0.8400,  0.7274],\n",
       "                       [-1.3186,  0.3422,  0.1518, -0.3433,  0.0974,  0.0467, -0.6545,  1.0999],\n",
       "                       [-0.4649,  0.5751, -1.3093,  1.4274, -1.4459,  0.6874, -0.4509, -0.1412],\n",
       "                       [-0.0166, -0.0761, -0.1564,  0.2834, -0.6170,  0.3350,  0.0094, -0.2923],\n",
       "                       [-0.0611, -0.4306,  0.7124, -1.4103,  1.5701, -1.6413,  0.6318, -0.1733],\n",
       "                       [ 1.3345, -0.8171, -0.7828,  1.3270, -1.1631,  0.1390,  0.1938, -0.9894],\n",
       "                       [-1.0072,  0.6329,  0.2257, -0.6394, -0.0790,  0.5386, -0.5446,  0.2823],\n",
       "                       [ 0.7120, -0.9184,  0.2759, -0.4077,  1.0179, -1.4287,  0.4994, -0.3726],\n",
       "                       [-1.3927,  0.0320, -0.0627, -0.1734, -0.3634,  0.9436, -2.2473,  2.2860],\n",
       "                       [ 1.7371, -2.0746,  1.2393, -0.6673, -0.3496, -0.1109,  0.8526, -1.7372],\n",
       "                       [-1.2188,  2.2521, -2.9095,  2.0538, -0.7915, -0.2774,  0.5127, -0.1629],\n",
       "                       [ 1.3441, -1.6899,  0.8750, -0.3083, -0.8456,  1.6445, -1.3545, -0.3032],\n",
       "                       [ 0.4370,  0.1799, -0.1652, -0.4760,  0.4889, -0.8791,  1.1183, -1.5514]],\n",
       "                      dtype=torch.float64)),\n",
       "              ('visible_bias',\n",
       "               tensor([-0.8276, -0.6780, -0.2126, -1.1056, -0.5621, -0.5322, -0.6863, -0.5310],\n",
       "                      dtype=torch.float64)),\n",
       "              ('hidden_bias',\n",
       "               tensor([ 0.0623,  0.0407,  0.0720, -0.0179,  0.0850, -0.0898,  0.0092,  0.2285,\n",
       "                        0.1476,  0.1681,  0.2023,  0.1266,  0.1105,  0.1025,  0.1671,  0.0360],\n",
       "                      dtype=torch.float64))])}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After training is complete, we save the parameters of the trained RBM below\n",
    "# as rydberg_data.pt in the output directory.\n",
    "nn_state.save(\"output/rydberg_data.pt\")\n",
    "torch.load(\"output/rydberg_data.pt\")\n",
    "# Below we have the parameters  (weights and biases) of the trained RBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have our trained RBM. Here we reconstruct the wavefunction by sampling from the RBM. \n",
    "# Let us see step by step how it works -\n",
    "\n",
    "# The first step is to sample from our trained RBM using QuCumber.\n",
    "# We then get num_samples samples which is stored in the variable 'samples'\n",
    "num_samples = 5000\n",
    "samples = nn_state.sample(num_samples = num_samples , k = 10)\n",
    "\n",
    "# Now we print out the samples in 'output/reconstructedSample.txt'\n",
    "sampleList = samples.tolist() # convert to list for convenience\n",
    "sampleList\n",
    "with open('output/reconstructedSample.txt', 'w') as fp:\n",
    "    for item in sampleList:\n",
    "        fp.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the samples we obtain the amplitudes of each of the basis states.\n",
    "# Note that here we have an 8 atom array. Hencem there are 2^8 basis states\n",
    "# which are: |00000000>, |00000001>, ..., |11111111>\n",
    "\n",
    "# Now say X is any of the states from {|00000000>, |00000001>, ..., |11111111>}\n",
    "# Now, the amplitude of a state X is computed by taking the\n",
    "# square root of the number of states X present in \n",
    "# our produced sample in in 'output/reconstructedSample.txt'.\n",
    "\n",
    "# Therefore the reconstructed sample is like: \n",
    "#    |psi> = amplitudeList[0] |00000000> + amplitudeList[1] |00000001> + ... + amplitudeList[255] |11111111>  \n",
    "# where amplitudeList is defined below.\n",
    "\n",
    "# The following functions below finds the amplitudes in the way described above, stores the \n",
    "# amplitudes in amplitudeList and then also \n",
    "# prints out the amplitudes in 'output/reconstructedSample.txt'\n",
    "\n",
    "def amplitude(sample):\n",
    "    return np.sqrt(sampleList.count(sample)/num_samples)\n",
    "\n",
    "def getBinaryString(i):\n",
    "    sites = nv # nv = 8\n",
    "    getbinary = lambda x, n: format(x, 'b').zfill(n)\n",
    "    tempStr = getbinary(i, sites)\n",
    "    return tempStr\n",
    "\n",
    "# Python code to convert string to list character-wise\n",
    "def ConvertToList(string):\n",
    "    list1=[]\n",
    "    list1[:0]=string\n",
    "    return list1\n",
    "\n",
    "amplitudeList = []\n",
    "def getAmplitude(sample):\n",
    "    for i in range (2**nv):\n",
    "        binaryString = getBinaryString(i)\n",
    "        strList = ConvertToList(binaryString)\n",
    "        tempStr = str(amplitude(list(map(int, strList))))\n",
    "        amplitudeList.append(tempStr)\n",
    "    return 0\n",
    "\n",
    "getAmplitude(sampleList)\n",
    "with open('output/reconstructedStateAmplitudes.txt', 'w') as fp:\n",
    "    for item in amplitudeList:\n",
    "        fp.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3904, 0.3604, 0.4166, 0.3382, 0.42, 0.3512, 0.3904, 0.3834]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So now we have obtained the reconstructed wavefunction.\n",
    "# Now in the 'data' directory for each delta we have 1_pt_fun and 2_pt_fn\n",
    "# 1_pt_fn is the one point function and \n",
    "# 2_pt_fn is the 2 point function.\n",
    "\n",
    "# 1_pt_fn is the average occupation of each site. Note that\n",
    "# there are 8 sites as there are 8 atoms. Occupation of each site\n",
    "# refers to the proportion of atoms in the up state (state 1) \n",
    "# in each state.\n",
    "\n",
    "# Now we get average occupation of each site by finding the \n",
    "# number of atoms in the up state in each site and then\n",
    "# by dividing by the number of the produced samples \n",
    "occupation = [0]*nv\n",
    "for i in range(len(sampleList)):\n",
    "    j = 0\n",
    "    for j in range(nv):\n",
    "        occupation[j] = sampleList[i][j] + occupation[j]\n",
    "for i in range(nv):\n",
    "    occupation[i] = occupation[i]/len(sampleList)\n",
    "\n",
    "# Thus the average occupation per site of each site for the reconstructed state is given below\n",
    "# This data can be compared to data/nY=8/δ=delta_1_pt_fn.csv for the relevant delta\n",
    "\n",
    "# (Note: The list in data/nY=8/δ=delta_1_pt_fn.csv was obtained from \n",
    "# the sample in data/nY=8/δ=delta_samples.csv)\n",
    "\n",
    "# Thus the data in 'occupation' and 'data/nY=8/δ=delta_1_pt_fn.csv' are expected to be close (which they are)\n",
    "occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.23798784, -0.11810016,  0.10655936, -0.08423328,  0.079432  ,\n",
       "        -0.09010848,  0.10358784, -0.12767936],\n",
       "       [-0.11810016,  0.23051184, -0.11974264,  0.10051272, -0.085368  ,\n",
       "         0.07282752, -0.08190016,  0.10942264],\n",
       "       [ 0.10655936, -0.11974264,  0.24304444, -0.11829412,  0.106628  ,\n",
       "        -0.08510992,  0.08115936, -0.09212444],\n",
       "       [-0.08423328,  0.10051272, -0.11829412,  0.22382076, -0.118044  ,\n",
       "         0.10742416, -0.08543328,  0.08453412],\n",
       "       [ 0.079432  , -0.085368  ,  0.106628  , -0.118044  ,  0.2436    ,\n",
       "        -0.121104  ,  0.114432  , -0.092428  ],\n",
       "       [-0.09010848,  0.07282752, -0.08510992,  0.10742416, -0.121104  ,\n",
       "         0.22785856, -0.11390848,  0.11254992],\n",
       "       [ 0.10358784, -0.08190016,  0.08115936, -0.08543328,  0.114432  ,\n",
       "        -0.11390848,  0.23798784, -0.12367936],\n",
       "       [-0.12767936,  0.10942264, -0.09212444,  0.08453412, -0.092428  ,\n",
       "         0.11254992, -0.12367936,  0.23640444]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we look at the 2 point function\n",
    "# 2 point function is the covariance matrix for occupations.\n",
    "# It is found by 2_pt_fn = outerProdT1 - OuterProdT2\n",
    "\n",
    "# outerProdT1 is the average of the outer product of the spin vectors with themselves\n",
    "# So we first find the sum of the outerproducts of the spin vectors with themselves.\n",
    "# Then we take the average\n",
    "outerProdT1 = np.zeros((nv,nv))\n",
    "for i in range(len(sampleList)):\n",
    "    outerProdT1 = outerProdT1 + np.outer(sampleList[i], sampleList[i])\n",
    "outerProdT1 = outerProdT1/len(sampleList)\n",
    "\n",
    "# outerProd2 is simply the outer product of the occupation vector\n",
    "# with itself. Note that we already found the occupation vector for the \n",
    "# one point function\n",
    "\n",
    "outerProdT2 = np.outer(occupation,occupation)\n",
    "outerProdT2\n",
    "\n",
    "#Hence we have:\n",
    "TwoPointFunc = outerProdT1 - outerProdT2\n",
    "\n",
    "# The TwoPointFunction data for the reconstructed state is given below.\n",
    "# This matrix can be compared to data/nY=8/δ=delta_2_pt_fn.csv for the relevant delta\n",
    "\n",
    "# (Note: The matrix in data/nY=8/δ=delta_2_pt_fn.csv was obtained from \n",
    "# the sample in data/nY=8/δ=delta_samples.csv)\n",
    "\n",
    "# Thus the matrix in 'TwoFuncPoint' and 'data/nY=8/δ=delta_2_pt_fn.csv' are expected \n",
    "# to be nearly equal (which they are)\n",
    "\n",
    "# Note that the matrix in data/nY=8/δ=delta_2_pt_fn.csv may be a bit misleading. \n",
    "# The matrix generated there is symmetric and this is why the values \n",
    "# below the diagonal were not computed. However the actual matrix is symmetric\n",
    "TwoPointFunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
